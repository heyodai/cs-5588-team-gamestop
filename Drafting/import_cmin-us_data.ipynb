{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "\n",
    "# Base URL for the raw files in the GitHub repo\n",
    "base_url = \"https://raw.githubusercontent.com/BigRoddy/CMIN-Dataset/main/CMIN-US/price/raw/\"\n",
    "\n",
    "# List of chosen stock CSV filenames\n",
    "csv_files = ['AAPL.csv', 'AMZN.csv', 'BAC.csv', 'BA.csv', 'CVX.csv', 'CAT.csv', 'DIS.csv', 'JNJ.csv', \n",
    "             'MCD.csv', 'MSFT.csv', 'NKE.csv', 'PEP.csv', 'TSLA.csv', 'TGT.csv', 'XOM.csv', 'UNH.csv',\n",
    "             'NEE.csv', 'RIO.csv', 'HD.csv', 'PG.csv']\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('cmin-us.db')\n",
    "\n",
    "# Create a new table that will hold all the data from the 20 CSV files\n",
    "for file in csv_files:\n",
    "    # Build the full URL to the CSV file\n",
    "    url = base_url + file\n",
    "    \n",
    "    # Download the CSV file\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Save the file locally\n",
    "    with open(file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Add a column for the stock ticker (from the filename)\n",
    "    df['Stock'] = file.split('.')[0]\n",
    "    \n",
    "    # Append the data to a single table in SQLite\n",
    "    df.to_sql('prices', conn, if_exists='append', index=False)\n",
    "\n",
    "# Close the SQLite connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date       Open       High        Low      Close  Adj Close  \\\n",
      "0  2018-01-02  42.540001  43.075001  42.314999  43.064999  41.188164   \n",
      "1  2018-01-03  43.132500  43.637501  42.990002  43.057499  41.180992   \n",
      "2  2018-01-04  43.134998  43.367500  43.020000  43.257500  41.372272   \n",
      "3  2018-01-05  43.360001  43.842499  43.262501  43.750000  41.843311   \n",
      "4  2018-01-08  43.587502  43.902500  43.482498  43.587502  41.687901   \n",
      "5  2018-01-09  43.637501  43.764999  43.352501  43.582500  41.683121   \n",
      "6  2018-01-10  43.290001  43.575001  43.250000  43.572498  41.673542   \n",
      "7  2018-01-11  43.647499  43.872501  43.622501  43.820000  41.910255   \n",
      "8  2018-01-12  44.044998  44.340000  43.912498  44.272499  42.343048   \n",
      "9  2018-01-16  44.474998  44.847500  44.035000  44.047501  42.127838   \n",
      "\n",
      "      Volume Stock  \n",
      "0  102223600  AAPL  \n",
      "1  118071600  AAPL  \n",
      "2   89738400  AAPL  \n",
      "3   94640000  AAPL  \n",
      "4   82271200  AAPL  \n",
      "5   86336000  AAPL  \n",
      "6   95839600  AAPL  \n",
      "7   74670800  AAPL  \n",
      "8  101672400  AAPL  \n",
      "9  118263600  AAPL  \n"
     ]
    }
   ],
   "source": [
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('cmin-us.db')\n",
    "\n",
    "# Query to fetch the first 10 rows from the 'prices' table\n",
    "query = \"SELECT * FROM prices LIMIT 10\"\n",
    "\n",
    "# Load the query result into a pandas DataFrame\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of chosen stock tickers\n",
    "stocks = ['AAPL', 'AMZN', 'BAC', 'BA', 'CVX', 'CAT', 'DIS', 'JNJ', \n",
    "          'MCD', 'MSFT', 'NKE', 'PEP', 'TSLA', 'TGT', 'XOM', 'UNH',\n",
    "          'NEE', 'RIO', 'HD', 'PG']\n",
    "\n",
    "# Base URL for the preprocessed news files in the GitHub repo\n",
    "news_base_url = \"https://api.github.com/repos/BigRoddy/CMIN-Dataset/contents/CMIN-US/news/preprocessed/\"\n",
    "\n",
    "# Function to retrieve file list for a given stock from GitHub API\n",
    "def get_available_files(stock):\n",
    "    url = news_base_url + stock\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        file_data = response.json()\n",
    "        # Extract file names (which represent dates)\n",
    "        available_files = [file['name'] for file in file_data if file['type'] == 'file']\n",
    "        return available_files\n",
    "    else:\n",
    "        print(f\"Failed to retrieve file list for {stock}: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Connect to the existing SQLite database\n",
    "conn = sqlite3.connect('cmin-us.db')\n",
    "\n",
    "# Loop through each stock\n",
    "for stock in stocks:\n",
    "    # Get the available file names (dates) for this stock\n",
    "    available_files = get_available_files(stock)\n",
    "    \n",
    "    for file_name in available_files:\n",
    "        # Build the raw content URL\n",
    "        file_url = f\"https://raw.githubusercontent.com/BigRoddy/CMIN-Dataset/main/CMIN-US/news/preprocessed/{stock}/{file_name}\"\n",
    "        \n",
    "        # Download and read the file content\n",
    "        response = requests.get(file_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Each line in the response is a JSON-like object\n",
    "            lines = response.text.splitlines()\n",
    "            \n",
    "            # Parse each line and create a list of records\n",
    "            news_data = []\n",
    "            for line in lines:\n",
    "                news_item = json.loads(line)  # The lines are JSON-like\n",
    "                news_data.append({\n",
    "                    'text': ' '.join(news_item['text']),  # Combine text into one string\n",
    "                    'created_at': news_item['created_at'],\n",
    "                    'date': file_name.replace('.json', ''),  # Use date from the file name\n",
    "                    'stock': stock  # Stock ticker from folder name\n",
    "                })\n",
    "            \n",
    "            # Convert the list of records into a DataFrame\n",
    "            df = pd.DataFrame(news_data)\n",
    "            \n",
    "            # Append the data to the SQLite table ('news' table)\n",
    "            df.to_sql('news_data', conn, if_exists='append', index=False)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for {stock} on {file_name}: {response.status_code}\")\n",
    "\n",
    "# Close the SQLite connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text           created_at  \\\n",
      "0  Forget Your iPhone X, Ignore The Samsung And P...  2018-01-01 23:28:00   \n",
      "1                  Apple's Executive Cash Bonus Plan  2018-01-01 22:19:00   \n",
      "2     2 Warren Buffett Stocks to Consider Buying Now  2018-01-01 21:08:20   \n",
      "3     2 Warren Buffett Stocks to Consider Buying Now  2018-01-01 20:02:00   \n",
      "4   Bitcoin or Stocks? Here’s the One to Buy in 2018  2018-01-01 16:06:40   \n",
      "5  Cramer reflects on how Trump's actions are fue...  2018-01-02 23:34:00   \n",
      "6  Tech Leads Market Advance To New Highs; These ...  2018-01-02 23:08:52   \n",
      "7  Tuesday Apple Rumors: Battery Replacement is A...  2018-01-02 23:07:18   \n",
      "8  Here&apos;s Why Optical Networking Stocks Fell...  2018-01-02 23:00:00   \n",
      "9             Why Apple Could Be a Big Buyer in 2018  2018-01-02 22:32:24   \n",
      "\n",
      "         date stock  \n",
      "0  2018-01-01  AAPL  \n",
      "1  2018-01-01  AAPL  \n",
      "2  2018-01-01  AAPL  \n",
      "3  2018-01-01  AAPL  \n",
      "4  2018-01-01  AAPL  \n",
      "5  2018-01-02  AAPL  \n",
      "6  2018-01-02  AAPL  \n",
      "7  2018-01-02  AAPL  \n",
      "8  2018-01-02  AAPL  \n",
      "9  2018-01-02  AAPL  \n"
     ]
    }
   ],
   "source": [
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('cmin-us.db')\n",
    "\n",
    "# Query to fetch the first 10 rows from the 'prices' table\n",
    "query = \"SELECT * FROM news_data LIMIT 10\"\n",
    "\n",
    "# Load the query result into a pandas DataFrame\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
