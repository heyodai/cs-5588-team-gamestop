{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /Users/namuunlkhagvadorj/Library/Python/3.9/lib/python/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/namuunlkhagvadorj/Library/Python/3.9/lib/python/site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in /Users/namuunlkhagvadorj/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/namuunlkhagvadorj/Library/Python/3.9/lib/python/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/namuunlkhagvadorj/Library/Python/3.9/lib/python/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/namuunlkhagvadorj/Library/Python/3.9/lib/python/site-packages (from requests) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/namuunlkhagvadorj/Library/Python/3.9/lib/python/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/namuunlkhagvadorj/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/namuunlkhagvadorj/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/namuunlkhagvadorj/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/namuunlkhagvadorj/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/namuunlkhagvadorj/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page successfully fetched\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "url = \"https://www.wsj.com/news/archive/2018/01/01?page=1\"\n",
    "\n",
    "# Add a User-Agent header to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Send GET request with headers\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Page successfully fetched\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<!--GRAND CANYON PREBID -->\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"Search WSJ's digital archive of news articles and top headlines from January 1, 2018\" name=\"description\"/>\n",
      "  <meta content=\"News, WSJ, Archives, News Archives, business news, news articles, markets news, world business, newspaper archives, headlines, today's news, yesterday's news\" name=\"keywords\"/>\n",
      "  <meta content=\"Archive\" name=\"page.section\"/>\n",
      "  <meta content=\"NewsArchive\" name=\"page.subsection\"/>\n",
      "  <meta content=\"wsj_newsarchive\" name=\"page.id\"/>\n",
      "  <meta content=\"wsj_newsarchive\" name=\"ad.id\"/>\n",
      "  <meta content=\"summary\" name=\"twitter:card\"/>\n",
      "  <meta content=\"The Wall Street Journals' News Archive for January 1, 2018\" name=\"twitter:title\"/>\n",
      "  <meta content=\"Search WSJ's digital archive of news articles and top headlines from January 1, 2018\" name=\"twitter:description\"/>\n",
      "  <meta content=\"https://s.wsj.net/img/meta/wsj-social-share.png\" name=\"twitter:image\"/>\n",
      "  <meta content\n"
     ]
    }
   ],
   "source": [
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Check the page structure by printing part of the HTML\n",
    "print(soup.prettify()[:1000])  # Print the first 1000 characters of the HTML structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Georgia Outguns Oklahoma in Rose Bowl Epic\n",
      "2. Pension Funds Ask: What to Buy When Nothing Is Cheap?\n",
      "3. In Cities With Low Unemployment, Wages Finally Start to Get Bigger\n",
      "4. A Browser You’ve Never Heard of Is Dethroning Google in Asia\n",
      "5. 2017 Marked Safest Year in Commercial Aviation History\n",
      "6. China Private Factory Gauge at Odds With Official PMI\n",
      "7. The Limits of Amazon\n",
      "8. When to Worry About a Bitcoin Bubble\n",
      "9. Regulatory Monitors Save Shareholders Money\n",
      "10. Menlo Therapeutics Files for $98 Million IPO\n",
      "11. Smaller Chinese Banks Brace for Tighter Oversight\n",
      "12. Europe Readies for Brexit, Mifid, Perhaps the Banking Union\n",
      "13. Congress to Tackle Dodd-Frank Rollback, Fannie-Freddie Overhaul\n",
      "14. Trump Backs Protesters in Iran\n",
      "15. Bitcoin a New Kind of Test for CFTC\n",
      "16. How the SEC Might Spur More IPOs\n",
      "17. Why 2018 Could Be a Good Year for Fintech in Washington\n",
      "18. What’s in Store for the CFPB Under Republican Leadership?\n",
      "19. Where’s the Line on Bank Deregulation?\n",
      "20. Costa Rica Probes Cause of Plane Crash That Killed Families\n",
      "21. Slow Start for Asia Stocks With Closings, Delays\n",
      "22. Oil Futures Reverse Early Drop in Asian Trading\n",
      "23. Five Things to Know About the Iranian Protests\n",
      "24. Photos of the Day: Jan. 1\n",
      "25. Playboy Might Kill Magazine to Focus on ‘World of Playboy’\n",
      "26. Trump Administration Urges International Support for Iran Protesters\n",
      "27. U.S. Hockey Team Turns to an Olympic Veteran\n",
      "28. New Year Brings Record Cold to Midwest\n",
      "29. Nick Saban’s Other Dynasty: His Pickup Basketball Team\n",
      "30. Free Trade Has Been a Boon for Energy Independence\n",
      "31. Mayor de Blasio Sworn In, Vowing To Champion Working New Yorkers\n",
      "32. About That Trump ‘Autocracy’\n",
      "33. Trump Gets the U.N. to Cut Spending\n",
      "34. High Taxes in New York Are Your Problem, Gov. Cuomo\n",
      "35. The Tax Bill Won’t Hurt Charitable Giving\n",
      "36. Trump’s National Security Strategy Misses Key Issues\n",
      "37. If We Can Put a Man on the Moon, Why Can’t We Put a Man on the Moon?\n",
      "38. What New Tax Law? Caterpillar Fights to Protect Its Swiss-Made Profits\n",
      "39. Push for New York Congestion Charge Picks Up Steam\n",
      "40. Death Toll Rises in Iran as Widespread Protests Continue\n",
      "41. Are Congress’s Russia Probes Nearly Done? Depends Which Party You Ask\n",
      "42. A Blockbuster and Online Fees Juice China’s Box Office\n",
      "43. New York Politics in 2018: What to Watch\n",
      "44. Fresh Fight Looms Over Aid to Homeless Veterans\n",
      "45. Eight Things to Watch in Markets in 2018\n",
      "46. Miners Look to Cash In on Cobalt Demand\n",
      "47. Donald Trump’s Watergate?\n",
      "48. Congress’s Gift to Blue-State Taxpayers\n",
      "49. Review: The Power of Prudence\n",
      "50. Iran’s Theocracy Is on the Brink\n"
     ]
    }
   ],
   "source": [
    "# Find all <h2> tags with the specific class containing the headlines\n",
    "headlines = []\n",
    "\n",
    "for h2_tag in soup.find_all('h2', class_='WSJTheme--headline--unZqjb45'):\n",
    "    headline = h2_tag.find('span', class_='WSJTheme--headlineText--He1ANr9C')\n",
    "    if headline:\n",
    "        headlines.append(headline.text.strip())  # Extract headline text\n",
    "\n",
    "# Print the extracted headlines\n",
    "for idx, headline in enumerate(headlines, 1):\n",
    "    print(f\"{idx}. {headline}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping headlines for 2018/02/01\n",
      "Scraping headlines for 2018/02/02\n",
      "Scraping headlines for 2018/02/03\n",
      "Scraping headlines for 2018/02/04\n",
      "Scraping headlines for 2018/02/05\n",
      "Scraping headlines for 2018/02/06\n",
      "Scraping headlines for 2018/02/07\n",
      "Scraping headlines for 2018/02/08\n",
      "Scraping headlines for 2018/02/09\n",
      "Scraping headlines for 2018/02/10\n",
      "Scraping headlines for 2018/02/11\n",
      "Scraping headlines for 2018/02/12\n",
      "Scraping headlines for 2018/02/13\n",
      "Scraping headlines for 2018/02/14\n",
      "Scraping headlines for 2018/02/15\n",
      "Scraping headlines for 2018/02/16\n",
      "Scraping headlines for 2018/02/17\n",
      "Scraping headlines for 2018/02/18\n",
      "Scraping headlines for 2018/02/19\n",
      "Scraping headlines for 2018/02/20\n",
      "Scraping headlines for 2018/02/21\n",
      "Scraping headlines for 2018/02/22\n",
      "Scraping headlines for 2018/02/23\n",
      "Scraping headlines for 2018/02/24\n",
      "Scraping headlines for 2018/02/25\n",
      "Scraping headlines for 2018/02/26\n",
      "Scraping headlines for 2018/02/27\n",
      "Scraping headlines for 2018/02/28\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Function to fetch and parse the headlines for a given date\n",
    "def fetch_headlines_for_date(date):\n",
    "    url = f\"https://www.wsj.com/news/archive/{date}?page=1\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # Send the request\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve {url}\")\n",
    "        return []\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract headlines and article links\n",
    "    articles = []\n",
    "    for h2_tag in soup.find_all('h2', class_='WSJTheme--headline--unZqjb45'):\n",
    "        headline = h2_tag.find('span', class_='WSJTheme--headlineText--He1ANr9C')\n",
    "        article_link_tag = h2_tag.find('a', href=True)\n",
    "        if headline and article_link_tag:\n",
    "            headline_text = headline.text.strip()\n",
    "            article_url = article_link_tag['href']\n",
    "            if article_url.startswith('/'):\n",
    "                article_url = 'https://www.wsj.com' + article_url\n",
    "            articles.append({\n",
    "                'date': date,\n",
    "                'headline': headline_text,\n",
    "                'url': article_url\n",
    "            })\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# Function to save the headlines to a CSV file\n",
    "def save_to_csv(data, start_date, end_date):\n",
    "    filename = f'/Users/namuunlkhagvadorj/Downloads/wsj_headlines_{start_date.strftime(\"%Y%m%d\")}_{end_date.strftime(\"%Y%m%d\")}.csv'\n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['date', 'headline', 'url']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        # Write the header only once\n",
    "        if csvfile.tell() == 0:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        writer.writerows(data)\n",
    "\n",
    "# Function to scrape headlines from 2018-01-01 to 2020-01-01\n",
    "def scrape_headlines(start_date, end_date):\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime('%Y/%m/%d')\n",
    "        print(f\"Scraping headlines for {date_str}\")\n",
    "        \n",
    "        # Fetch headlines for the current date\n",
    "        headlines = fetch_headlines_for_date(date_str)\n",
    "        \n",
    "        if headlines:\n",
    "            save_to_csv(headlines, start_date, end_date)\n",
    "        \n",
    "        # Move to the next day\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "# Set the date range from 2018-01-01 to 2020-01-01\n",
    "start_date = datetime(2018, 2, 1)\n",
    "end_date = datetime(2018, 2, 28)\n",
    "\n",
    "# Create a CSV file and scrape headlines for each day in the range\n",
    "scrape_headlines(start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
